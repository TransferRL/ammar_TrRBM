{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-21 00:34:15,026] Making new env: MountainCar-v0\n",
      "[2017-11-21 00:34:15,037] Making new env: CartPole-v1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from envs import ENVS_DICTIONARY\n",
    "\n",
    "sys.path.append('../baselines/baselines/')\n",
    "\n",
    "import deepq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = ENVS_DICTIONARY['3DMountainCar']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import baselines.common.tf_util as U\n",
    "from baselines import logger\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*Copied from OpenAI Baselines deeq implementation\n",
    "https://github.com/TransferRL/baselines/blob/master/baselines/deepq/simple.py\n",
    "\"\"\"\n",
    "\n",
    "class ActWrapper(object):\n",
    "    def __init__(self, act, act_params):\n",
    "        self._act = act\n",
    "        self._act_params = act_params\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data, act_params = cloudpickle.load(f)\n",
    "        act = deepq.build_act(**act_params)\n",
    "        sess = tf.Session()\n",
    "        sess.__enter__()\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            arc_path = os.path.join(td, \"packed.zip\")\n",
    "            with open(arc_path, \"wb\") as f:\n",
    "                f.write(model_data)\n",
    "\n",
    "            zipfile.ZipFile(arc_path, 'r', zipfile.ZIP_DEFLATED).extractall(td)\n",
    "            U.load_state(os.path.join(td, \"model\"))\n",
    "\n",
    "        return ActWrapper(act, act_params)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._act(*args, **kwargs)\n",
    "\n",
    "    def save(self, path=None):\n",
    "        \"\"\"Save model to a pickle located at `path`\"\"\"\n",
    "        if path is None:\n",
    "            path = os.path.join(logger.get_dir(), \"model.pkl\")\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            U.save_state(os.path.join(td, \"model\"))\n",
    "            arc_name = os.path.join(td, \"packed.zip\")\n",
    "            with zipfile.ZipFile(arc_name, 'w') as zipf:\n",
    "                for root, dirs, files in os.walk(td):\n",
    "                    for fname in files:\n",
    "                        file_path = os.path.join(root, fname)\n",
    "                        if file_path != arc_name:\n",
    "                            zipf.write(file_path, os.path.relpath(file_path, td))\n",
    "            with open(arc_name, \"rb\") as f:\n",
    "                model_data = f.read()\n",
    "        with open(path, \"wb\") as f:\n",
    "            cloudpickle.dump((model_data, self._act_params), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*All code below copied and MODIFIED from OpenAI Baselines deeq implementation\n",
    "https://github.com/TransferRL/baselines/blob/master/baselines/deepq/simple.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DeepQ(object):\n",
    "    \"\"\"Train a deepq model.\n",
    "\n",
    "    Parameters\n",
    "    -------\n",
    "    env: gym.Env\n",
    "        environment to train on\n",
    "    q_func: (tf.Variable, int, str, bool) -> tf.Variable\n",
    "        the model that takes the following inputs:\n",
    "            observation_in: object\n",
    "                the output of observation placeholder\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "            scope: str\n",
    "            reuse: bool\n",
    "                should be passed to outer variable scope\n",
    "        and returns a tensor of shape (batch_size, num_actions) with values of every action.\n",
    "    lr: float\n",
    "        learning rate for adam optimizer\n",
    "    max_timesteps: int\n",
    "        number of env steps to optimizer for\n",
    "    buffer_size: int\n",
    "        size of the replay buffer\n",
    "    exploration_fraction: float\n",
    "        fraction of entire training period over which the exploration rate is annealed\n",
    "    exploration_final_eps: float\n",
    "        final value of random action probability\n",
    "    train_freq: int\n",
    "        update the model every `train_freq` steps.\n",
    "        set to None to disable printing\n",
    "    batch_size: int\n",
    "        size of a batched sampled from replay buffer for training\n",
    "    print_freq: int\n",
    "        how often to print out training progress\n",
    "        set to None to disable printing\n",
    "    checkpoint_freq: int\n",
    "        how often to save the model. This is so that the best version is restored\n",
    "        at the end of the training. If you do not wish to restore the best version at\n",
    "        the end of the training set this variable to None.\n",
    "    learning_starts: int\n",
    "        how many steps of the model to collect transitions for before learning starts\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    target_network_update_freq: int\n",
    "        update the target network every `target_network_update_freq` steps.\n",
    "    prioritized_replay: True\n",
    "        if True prioritized replay buffer will be used.\n",
    "    prioritized_replay_alpha: float\n",
    "        alpha parameter for prioritized replay buffer\n",
    "    prioritized_replay_beta0: float\n",
    "        initial value of beta for prioritized replay buffer\n",
    "    prioritized_replay_beta_iters: int\n",
    "        number of iterations over which beta will be annealed from initial value\n",
    "        to 1.0. If set to None equals to max_timesteps.\n",
    "    prioritized_replay_eps: float\n",
    "        epsilon to add to the TD errors when updating priorities.\n",
    "    callback: (locals, globals) -> None\n",
    "        function called at every steps with state of the algorithm.\n",
    "        If callback returns true training stops.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    act: ActWrapper\n",
    "        Wrapper over act function. Adds ability to save it and load it.\n",
    "        See header of baselines/deepq/categorical.py for details on the act function.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "              env,\n",
    "              q_func,\n",
    "              lr=5e-4,\n",
    "              max_timesteps=100000,\n",
    "              buffer_size=50000,\n",
    "              exploration_fraction=0.1,\n",
    "              exploration_final_eps=0.02,\n",
    "              train_freq=1,\n",
    "              batch_size=32,\n",
    "              print_freq=100,\n",
    "              checkpoint_freq=10000,\n",
    "              learning_starts=1000,\n",
    "              gamma=1.0,\n",
    "              target_network_update_freq=500,\n",
    "              prioritized_replay=False,\n",
    "              prioritized_replay_alpha=0.6,\n",
    "              prioritized_replay_beta0=0.4,\n",
    "              prioritized_replay_beta_iters=None,\n",
    "              prioritized_replay_eps=1e-6,\n",
    "              param_noise=False,\n",
    "              callback=None):\n",
    "\n",
    "        self.env = env\n",
    "        self.q_func=q_func\n",
    "        self.lr=lr\n",
    "        self.max_timesteps=max_timesteps\n",
    "        self.buffer_size=buffer_size\n",
    "        self.exploration_fraction=exploration_fraction\n",
    "        self.exploration_final_eps=exploration_final_eps\n",
    "        self.train_freq=train_freq\n",
    "        self.batch_size=batch_size\n",
    "        self.print_freq=print_freq\n",
    "        self.checkpoint_freq=checkpoint_freq\n",
    "        self.learning_starts=learning_starts\n",
    "        self.gamma=gamma\n",
    "        self.target_network_update_freq=target_network_update_freq\n",
    "        self.prioritized_replay=prioritized_replay\n",
    "        self.prioritized_replay_alpha=prioritized_replay_alpha\n",
    "        self.prioritized_replay_beta0=prioritized_replay_beta0\n",
    "        self.prioritized_replay_beta_iters=prioritized_replay_beta_iters\n",
    "        self.prioritized_replay_eps=prioritized_replay_eps\n",
    "        self.param_noise=param_noise\n",
    "        self.callback=callback\n",
    "    \n",
    "        # Create all the functions necessary to train the model\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.__enter__()\n",
    "\n",
    "        # capture the shape outside the closure so that the env object is not serialized\n",
    "        # by cloudpickle when serializing make_obs_ph\n",
    "        self.observation_space_shape = env.observation_space.shape\n",
    "    \n",
    "    def make_obs_ph(self,name):\n",
    "        return U.BatchInput(self.observation_space_shape, name=name)\n",
    "\n",
    "    def make_build_train(self):\n",
    "        # Build act and train networks\n",
    "        self.act, self.train, self.update_target, self.debug = deepq.build_train(\n",
    "            make_obs_ph=self.make_obs_ph,\n",
    "            q_func=self.q_func,\n",
    "            num_actions=self.env.action_space.n,\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=self.lr),\n",
    "            gamma=self.gamma,\n",
    "            grad_norm_clipping=10,\n",
    "            param_noise=self.param_noise\n",
    "        )\n",
    "\n",
    "        self.act_params = {\n",
    "            'make_obs_ph': self.make_obs_ph,\n",
    "            'q_func': self.q_func,\n",
    "            'num_actions': self.env.action_space.n,\n",
    "        }\n",
    "\n",
    "        self.act = ActWrapper(self.act, self.act_params)\n",
    "\n",
    "        # Create the replay buffer\n",
    "        if self.prioritized_replay:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(self.buffer_size, alpha=self.prioritized_replay_alpha)\n",
    "            if self.prioritized_replay_beta_iters is None:\n",
    "                self.prioritized_replay_beta_iters = self.max_timesteps\n",
    "            self.beta_schedule = LinearSchedule(self.prioritized_replay_beta_iters,\n",
    "                                           initial_p=self.prioritized_replay_beta0,\n",
    "                                           final_p=1.0)\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "            self.beta_schedule = None\n",
    "        # Create the schedule for exploration starting from 1.\n",
    "        self.exploration = LinearSchedule(schedule_timesteps=int(self.exploration_fraction * self.max_timesteps),\n",
    "                                     initial_p=1.0,\n",
    "                                     final_p=self.exploration_final_eps)\n",
    "\n",
    "        # Initialize the parameters and copy them to the target network.\n",
    "        U.initialize()\n",
    "        self.update_target()\n",
    "        \n",
    "        return 'make_build_train() complete'\n",
    "    \n",
    "    def transfer_pretrain(self, \n",
    "                          transferred_instances\n",
    "                         ,epochs\n",
    "                         ,store_in_replay_buffer=True\n",
    "                         ,batch_size=None\n",
    "                         ):\n",
    "        \"\"\"\n",
    "        This is a custom function from University of Toronto group to first pretrain\n",
    "        the deepq train network with transferred instances. These instances must be\n",
    "        (s,a,s',r) tuples mapped over to the same state and action spaces as the target\n",
    "        task environment.\n",
    "        \n",
    "        No output - just updates parameters of train and target networks.\n",
    "        \"\"\"\n",
    "        # TODO - function that trains self.act and self.train using mapped instances\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def task_train(self):\n",
    "        self.episode_rewards = [0.0]\n",
    "        self.episode_steps = [0.0]\n",
    "        self.saved_mean_reward = None\n",
    "        obs = self.env.reset()\n",
    "        reset = True\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            model_saved = False\n",
    "            model_file = os.path.join(td, \"model\")\n",
    "            for t in range(self.max_timesteps):\n",
    "                if self.callback is not None:\n",
    "                    if self.callback(locals(), globals()):\n",
    "                        break\n",
    "                # Take action and update exploration to the newest value\n",
    "                kwargs = {}\n",
    "                if not self.param_noise:\n",
    "                    update_eps = self.exploration.value(t)\n",
    "                    update_param_noise_threshold = 0.\n",
    "                else:\n",
    "                    update_eps = 0.\n",
    "                    # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
    "                    # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
    "                    # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
    "                    # for detailed explanation.\n",
    "                    update_param_noise_threshold = -np.log(1. - self.exploration.value(t) + self.exploration.value(t) / float(self.env.action_space.n))\n",
    "                    kwargs['reset'] = reset\n",
    "                    kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                    kwargs['update_param_noise_scale'] = True\n",
    "                action = self.act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]\n",
    "                env_action = action\n",
    "                reset = False\n",
    "                new_obs, rew, done, _ = self.env.step(env_action)\n",
    "                # Store transition in the replay buffer.\n",
    "                self.replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "                obs = new_obs\n",
    "\n",
    "                self.episode_rewards[-1] += rew\n",
    "                self.episode_steps[-1] += 1\n",
    "                if done:\n",
    "                    obs = self.env.reset()\n",
    "                    self.episode_rewards.append(0.0)\n",
    "                    self.episode_steps.append(0.0)\n",
    "                    reset = True\n",
    "\n",
    "                if t > self.learning_starts and t % self.train_freq == 0:\n",
    "                    # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "                    if self.prioritized_replay:\n",
    "                        experience = self.replay_buffer.sample(self.batch_size, beta=self.beta_schedule.value(t))\n",
    "                        (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "                    else:\n",
    "                        obses_t, actions, rewards, obses_tp1, dones = self.replay_buffer.sample(self.batch_size)\n",
    "                        weights, batch_idxes = np.ones_like(rewards), None\n",
    "                    td_errors = self.train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "                    if self.prioritized_replay:\n",
    "                        new_priorities = np.abs(td_errors) + self.prioritized_replay_eps\n",
    "                        self.replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "\n",
    "                if t > self.learning_starts and t % self.target_network_update_freq == 0:\n",
    "                    # Update target network periodically.\n",
    "                    self.update_target()\n",
    "\n",
    "                mean_100ep_reward = round(np.mean(self.episode_rewards[-101:-1]), 1)\n",
    "                num_episodes = len(self.episode_rewards)\n",
    "                if done and self.print_freq is not None and len(self.episode_rewards) % self.print_freq == 0:\n",
    "                    logger.record_tabular(\"steps\", t)\n",
    "                    logger.record_tabular(\"episodes\", num_episodes)\n",
    "                    logger.record_tabular(\"mean 100 episode reward\", mean_100ep_reward)\n",
    "                    logger.record_tabular(\"% time spent exploring\", int(100 * self.exploration.value(t)))\n",
    "                    logger.dump_tabular()\n",
    "\n",
    "                if (self.checkpoint_freq is not None and t > self.learning_starts and\n",
    "                        num_episodes > 100 and t % self.checkpoint_freq == 0):\n",
    "                    if self.saved_mean_reward is None or mean_100ep_reward > self.saved_mean_reward:\n",
    "                        if self.print_freq is not None:\n",
    "                            logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
    "                                       self.saved_mean_reward, mean_100ep_reward))\n",
    "                        U.save_state(model_file)\n",
    "                        model_saved = True\n",
    "                        self.saved_mean_reward = mean_100ep_reward\n",
    "            if model_saved:\n",
    "                if self.print_freq is not None:\n",
    "                    logger.log(\"Restored model with mean reward: {}\".format(self.saved_mean_reward))\n",
    "                U.load_state(model_file)\n",
    "\n",
    "        return self.act, self.episode_rewards, self.episode_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = deepq.models.mlp([64], layer_norm=True)\n",
    "dq = DeepQ(\n",
    "        env,\n",
    "        q_func=model,\n",
    "        lr=1e-3,\n",
    "        max_timesteps=100000,\n",
    "        buffer_size=50000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.1,\n",
    "        print_freq=10,\n",
    "        param_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make_build_train() complete'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.make_build_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "//anaconda/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| % time spent exploring  | 9         |\n",
      "| episodes                | 10        |\n",
      "| mean 100 episode reward | -2.97e+03 |\n",
      "| steps                   | 26695     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 9         |\n",
      "| episodes                | 20        |\n",
      "| mean 100 episode reward | -1.65e+03 |\n",
      "| steps                   | 31410     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 9         |\n",
      "| episodes                | 30        |\n",
      "| mean 100 episode reward | -1.56e+03 |\n",
      "| steps                   | 45179     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 9         |\n",
      "| episodes                | 40        |\n",
      "| mean 100 episode reward | -1.26e+03 |\n",
      "| steps                   | 49017     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| % time spent exploring  | 9         |\n",
      "| episodes                | 50        |\n",
      "| mean 100 episode reward | -1.09e+03 |\n",
      "| steps                   | 53233     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | -975     |\n",
      "| steps                   | 57528    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | -882     |\n",
      "| steps                   | 60859    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | -810     |\n",
      "| steps                   | 63949    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | -756     |\n",
      "| steps                   | 67274    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -717     |\n",
      "| steps                   | 70961    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | -478     |\n",
      "| steps                   | 74543    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> -497.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | -509     |\n",
      "| steps                   | 82306    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | -411     |\n",
      "| steps                   | 86258    |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: -497.3 -> -414.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | -426     |\n",
      "| steps                   | 91611    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | -450     |\n",
      "| steps                   | 98188    |\n",
      "--------------------------------------\n",
      "Restored model with mean reward: -414.8\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/9v/zklyctlj4w7c3dz5p_84zkgh0000gn/T/tmpesxlpfrc/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-21 00:45:06,394] Restoring parameters from /var/folders/9v/zklyctlj4w7c3dz5p_84zkgh0000gn/T/tmpesxlpfrc/model\n"
     ]
    }
   ],
   "source": [
    "act, episode_rewards, episode_steps = dq.task_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12145ff60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(range(len(episode_steps[:50])),episode_steps[:50])\n",
    "plt.title('steps per episode during training - no transfer')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15925.0,\n",
       " 448.0,\n",
       " 491.0,\n",
       " 348.0,\n",
       " 2009.0,\n",
       " 1184.0,\n",
       " 778.0,\n",
       " 4926.0,\n",
       " 587.0,\n",
       " 648.0,\n",
       " 561.0,\n",
       " 483.0,\n",
       " 524.0,\n",
       " 424.0,\n",
       " 310.0,\n",
       " 491.0,\n",
       " 442.0,\n",
       " 445.0,\n",
       " 387.0,\n",
       " 436.0,\n",
       " 324.0,\n",
       " 9995.0,\n",
       " 687.0,\n",
       " 328.0,\n",
       " 359.0,\n",
       " 379.0,\n",
       " 349.0,\n",
       " 358.0,\n",
       " 554.0,\n",
       " 331.0,\n",
       " 380.0,\n",
       " 285.0,\n",
       " 299.0,\n",
       " 849.0,\n",
       " 404.0,\n",
       " 297.0,\n",
       " 412.0,\n",
       " 328.0,\n",
       " 253.0,\n",
       " 354.0,\n",
       " 318.0,\n",
       " 310.0,\n",
       " 431.0,\n",
       " 305.0,\n",
       " 458.0,\n",
       " 356.0,\n",
       " 634.0,\n",
       " 538.0,\n",
       " 512.0,\n",
       " 442.0,\n",
       " 698.0,\n",
       " 528.0,\n",
       " 457.0,\n",
       " 257.0,\n",
       " 220.0,\n",
       " 326.0,\n",
       " 237.0,\n",
       " 789.0,\n",
       " 341.0,\n",
       " 295.0,\n",
       " 254.0,\n",
       " 238.0,\n",
       " 311.0,\n",
       " 356.0,\n",
       " 391.0,\n",
       " 332.0,\n",
       " 409.0,\n",
       " 277.0,\n",
       " 468.0,\n",
       " 375.0,\n",
       " 260.0,\n",
       " 256.0,\n",
       " 379.0,\n",
       " 319.0,\n",
       " 278.0,\n",
       " 318.0,\n",
       " 290.0,\n",
       " 299.0,\n",
       " 316.0,\n",
       " 281.0,\n",
       " 267.0,\n",
       " 323.0,\n",
       " 324.0,\n",
       " 310.0,\n",
       " 307.0,\n",
       " 303.0,\n",
       " 415.0,\n",
       " 370.0,\n",
       " 425.0,\n",
       " 400.0,\n",
       " 388.0,\n",
       " 310.0,\n",
       " 379.0,\n",
       " 362.0,\n",
       " 379.0,\n",
       " 331.0,\n",
       " 421.0,\n",
       " 365.0,\n",
       " 352.0,\n",
       " 283.0,\n",
       " 347.0,\n",
       " 340.0,\n",
       " 338.0,\n",
       " 342.0,\n",
       " 335.0,\n",
       " 340.0,\n",
       " 562.0,\n",
       " 385.0,\n",
       " 310.0,\n",
       " 826.0,\n",
       " 1022.0,\n",
       " 550.0,\n",
       " 520.0,\n",
       " 794.0,\n",
       " 1046.0,\n",
       " 565.0,\n",
       " 1029.0,\n",
       " 1170.0,\n",
       " 241.0,\n",
       " 308.0,\n",
       " 342.0,\n",
       " 514.0,\n",
       " 478.0,\n",
       " 365.0,\n",
       " 393.0,\n",
       " 306.0,\n",
       " 474.0,\n",
       " 349.0,\n",
       " 423.0,\n",
       " 430.0,\n",
       " 303.0,\n",
       " 512.0,\n",
       " 337.0,\n",
       " 780.0,\n",
       " 296.0,\n",
       " 589.0,\n",
       " 580.0,\n",
       " 780.0,\n",
       " 746.0,\n",
       " 995.0,\n",
       " 445.0,\n",
       " 510.0,\n",
       " 794.0,\n",
       " 773.0,\n",
       " 284.0,\n",
       " 541.0,\n",
       " 502.0,\n",
       " 923.0,\n",
       " 810.0,\n",
       " 321.0,\n",
       " 890.0,\n",
       " 600.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
